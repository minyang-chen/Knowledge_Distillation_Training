# Knowledge_Distillation_Training
employ knowledge distillation to compress their large deep models into lightweight versions (Teacher and Student Model)
